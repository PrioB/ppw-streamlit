{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb62725c",
   "metadata": {},
   "source": [
    "# Tugas 2\n",
    "\n",
    "Prio Budi Laksono\n",
    "\n",
    "210411100177\n",
    "\n",
    "Preprocessing hasil crawling data dari jatim.tribunnews.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39820b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menambang kategori Travel...\n",
      "Mengambil: https://jatim.tribunnews.com/travel?page=1\n",
      "Kesalahan saat mengambil https://jatim.tribunnews.com/travel?page=1: 403 Client Error: Forbidden for url: https://jatim.tribunnews.com/travel?page=1\n",
      "Menambang kategori Sport...\n",
      "Mengambil: https://jatim.tribunnews.com/sport?page=1\n",
      "Kesalahan saat mengambil https://jatim.tribunnews.com/sport?page=1: 403 Client Error: Forbidden for url: https://jatim.tribunnews.com/sport?page=1\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Proses penambangan data selesai, data tersimpan dalam 'tribunnews_articles.csv' dan 10 data pertama ditampilkan.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Fungsi untuk mendapatkan konten dari URL\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Kesalahan saat mengambil {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fungsi untuk mendapatkan detail artikel dari halaman detail\n",
    "def get_article_details(detail_url):\n",
    "    detail_soup = get_soup(detail_url)\n",
    "    if detail_soup:\n",
    "        # Ambil isi berita\n",
    "        content = ' '.join([p.text for p in detail_soup.find_all('p')])\n",
    "\n",
    "        # Ambil tanggal publikasi\n",
    "        date_tag = detail_soup.find('time')\n",
    "        date = date_tag.text.strip() if date_tag else 'Tidak ada tanggal'\n",
    "\n",
    "        # Ambil judul berita\n",
    "        title_tag = detail_soup.find('h1')\n",
    "        title = title_tag.text.strip() if title_tag else 'Tidak ada judul'\n",
    "\n",
    "        # Ambil kategori dari breadcrumb\n",
    "        breadcrumb = detail_soup.find('ul', {'class': 'breadcrumb'})\n",
    "        category = breadcrumb.find_all('li')[-1].find('span').text.strip() if breadcrumb else 'Tidak ada kategori'\n",
    "\n",
    "        return {\n",
    "            'judul': title,\n",
    "            'isi_berita': content,\n",
    "            'tanggal': date,\n",
    "            'kategori': category,\n",
    "            'url': detail_url\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Fungsi untuk mendapatkan artikel dari suatu kategori\n",
    "def get_articles(category_url, category_name, max_articles=100):\n",
    "    articles = []\n",
    "    page = 1\n",
    "    \n",
    "    while len(articles) < max_articles:\n",
    "        url = f'{category_url}?page={page}'  # Periksa pola yang benar untuk paginasi\n",
    "        print(f\"Mengambil: {url}\")  # Output debug\n",
    "        soup = get_soup(url)\n",
    "        \n",
    "        if soup is None:\n",
    "            break\n",
    "        \n",
    "        # Cari artikel di halaman\n",
    "        article_list = soup.find_all('h3')  # Berdasarkan tata letak yang diamati\n",
    "        \n",
    "        if not article_list:\n",
    "            print(f\"Tidak ada artikel ditemukan di halaman {page}.\")\n",
    "            break\n",
    "        \n",
    "        for article in article_list:\n",
    "            if len(articles) >= max_articles:\n",
    "                break\n",
    "            \n",
    "            # Ambil URL detail artikel\n",
    "            title_tag = article.find('a')\n",
    "            detail_url = title_tag['href'] if title_tag else None\n",
    "            \n",
    "            if detail_url:\n",
    "                if not detail_url.startswith('http'):\n",
    "                    detail_url = f'https://jatim.tribunnews.com{detail_url}'\n",
    "                # Buka halaman detail artikel\n",
    "                article_details = get_article_details(detail_url)\n",
    "                if article_details:\n",
    "                    # Tambahkan ke daftar artikel\n",
    "                    articles.append(article_details)\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(2)  # Beri jeda agar tidak terlalu cepat melakukan permintaan\n",
    "        \n",
    "    return articles\n",
    "\n",
    "# URL Kategori\n",
    "categories = {\n",
    "    'Travel': 'https://jatim.tribunnews.com/travel',\n",
    "    'Sport': 'https://jatim.tribunnews.com/sport'\n",
    "}\n",
    "\n",
    "# Mengumpulkan semua data\n",
    "all_articles = []\n",
    "for category_name, category_url in categories.items():\n",
    "    print(f\"Menambang kategori {category_name}...\")\n",
    "    articles = get_articles(category_url, category_name, max_articles=100)\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "# Simpan ke dalam DataFrame\n",
    "df = pd.DataFrame(all_articles)\n",
    "\n",
    "# Simpan ke dalam file CSV\n",
    "df.to_csv('tribunnews_articles.csv', index=False)\n",
    "\n",
    "# Tampilkan 10 data pertama dalam bentuk tabel\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"Proses penambangan data selesai, data tersimpan dalam 'tribunnews_articles.csv' dan 10 data pertama ditampilkan.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a6d13a",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtribunnews_articles.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"tribunnews_articles.csv\")\n",
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed4291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tribunnews_articles.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea7e4d",
   "metadata": {},
   "source": [
    "## **Preprocessing**\n",
    "\n",
    "Preprocessing adalah proses membersihkan dan mempersiapkan data mentah agar siap digunakan oleh model machine learning. Ini meliputi penanganan data yang hilang, normalisasi, mengubah data kategori menjadi angka, dan membersihkan teks. Tujuannya agar data lebih mudah dipahami dan diolah oleh model untuk hasil yang lebih akurat, Berikut adalah beberapa langkah umum dalam pre-processing teks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b900c6",
   "metadata": {},
   "source": [
    "### Cleansing\n",
    "\n",
    "Proses cleansing data adalah tahap pembersihan teks dari elemen-elemen yang tidak relevan terhadap hasil klasifikasi sentimen. Beberapa komponen yang tidak berpengaruh terhadap sentimen, seperti URL, tag HTML, emoji, simbol, angka, dan tanda baca (~!@#$%^&*{}<>:|), dihapus dari dokumen ulasan. Elemen-elemen tersebut dihilangkan untuk mengurangi kebisingan (noise) dalam data. Setelah dihapus, elemen ini digantikan dengan spasi agar struktur kalimat tetap terjaga. Dengan demikian, data menjadi lebih fokus pada kata-kata yang relevan untuk menentukan sentimen, sehingga membantu meningkatkan akurasi model prediksi sentimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def remove_url(text):\n",
    "    #Fungsi untuk menghapus URL dari teks.\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    #Fungsi untuk menghapus tag HTML dari teks.\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "\n",
    "    #Fungsi untuk menghapus emoji dari teks.\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emotikon wajah\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # simbol & gambar\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transportasi & simbol\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # bendera negara\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    #Fungsi untuk menghapus angka dari teks.\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_symbols(text):\n",
    "    #Fungsi untuk menghapus simbol dan karakter khusus dari teks.\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Asumsikan df adalah DataFrame yang berisi data CNN (judul, berita, tanggal, kategori)\n",
    "# Contoh: df = pd.read_csv('berita-cnn.csv')\n",
    "\n",
    "# Terapkan fungsi cleansing untuk kolom 'berita'\n",
    "df['berita_clean'] = df['isi_berita'].apply(remove_url)\n",
    "df['berita_clean'] = df['berita_clean'].apply(remove_html)\n",
    "df['berita_clean'] = df['berita_clean'].apply(remove_emoji)\n",
    "df['berita_clean'] = df['berita_clean'].apply(remove_symbols)\n",
    "df['berita_clean'] = df['berita_clean'].apply(remove_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Tampilkan beberapa baris dari hasil yang sudah dibersihkan\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de084e6",
   "metadata": {},
   "source": [
    "### CASE FOLDING\n",
    "\n",
    "Pada tahap case folding, semua huruf kapital dalam dokumen ulasan diubah menjadi huruf kecil, atau disebut lowercase. Tujuan dari langkah ini adalah untuk menghilangkan redundansi data yang hanya disebabkan oleh perbedaan kapitalisasi. Misalnya, kata \"Ekonomi\" dan \"ekonomi\" secara teknis sama dalam analisis teks, namun tanpa case folding, komputer akan menganggapnya berbeda. Dengan mengonversi seluruh teks menjadi huruf kecil, semua variasi penulisan diseragamkan, sehingga mencegah duplikasi penghitungan atau kesalahan dalam interpretasi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43478f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(text):\n",
    "    if isinstance(text, str):\n",
    "      lowercase_text = text.lower()\n",
    "      return lowercase_text\n",
    "    else :\n",
    "      return text\n",
    "\n",
    "df ['case_folding'] = df['berita_clean'].apply(case_folding)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ca053",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "\n",
    "Tokenization adalah tahap di mana setiap kata dalam sebuah dokumen dipecah menjadi unit-unit kata yang lebih kecil, atau disebut token. Proses ini memisahkan kata-kata berdasarkan spasi, sehingga setiap kata yang terpisah oleh spasi dianggap sebagai token tersendiri. Sebagai contoh, kalimat \"Upaya agar ekonomi stabil\" akan diuraikan menjadi token [\"Upaya\", \"agar\", \"ekonomi\", \"stabil\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89adaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['tokenize'] = df['case_folding'].apply(tokenize)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f0960",
   "metadata": {},
   "source": [
    "### STOPWORD REMOVAL\n",
    "\n",
    "Stopword removal adalah proses menghapus kata-kata yang dianggap tidak penting atau tidak memiliki makna signifikan dalam analisis teks, seperti \"dan,\" \"di,\" \"yang,\" atau \"itu.\" Kata-kata ini sering muncul dalam kalimat tetapi tidak memberikan informasi penting untuk pemrosesan atau analisis lebih lanjut. Dengan menghapus stopwords, data teks menjadi lebih ringkas dan fokus hanya pada kata-kata yang memiliki bobot lebih besar dalam analisis, seperti saat melakukan klasifikasi atau pemodelan teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "  return [word for word in text if word not in stop_words]\n",
    "\n",
    "df['stopword_removal'] = df['tokenize'].apply(lambda x: ' '.join(remove_stopwords(x)))\n",
    "\n",
    "\n",
    "df.to_csv(\"preprocessing-cnnnews.csv\", encoding='utf8', index=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c9282",
   "metadata": {},
   "source": [
    "## **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "TF-IDF adalah metode statistik yang digunakan untuk mengevaluasi pentingnya suatu kata dalam sebuah dokumen relatif terhadap koleksi dokumen lainnya. TF-IDF sering digunakan dalam tugas seperti penggalian teks, penambangan informasi, dan pemodelan pembelajaran mesin berbasis teks.\n",
    "Term Frequency (TF), yang menghitung seberapa sering sebuah kata muncul dalam dokumen, dan Inverse Document Frequency (IDF), yang menilai seberapa jarang kata tersebut muncul di seluruh dokumen dalam koleksi.\n",
    "\n",
    "TF-IDF bekerja dengan memberikan bobot lebih tinggi pada kata-kata yang sering muncul dalam sebuah dokumen, tetapi jarang muncul di dokumen lain, sehingga membantu mengidentifikasi kata-kata yang paling relevan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad19d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv(\"preprocessing-cnnnews.csv\")\n",
    "\n",
    "# Menginisialisasi TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Menghitung TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(df['stopword_removal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah hasilnya menjadi DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menginisialisasi TfidfVectorizer dengan normalisasi L2\n",
    "vectorizer = TfidfVectorizer(norm='l2')\n",
    "\n",
    "# Menghitung TF-IDF dengan normalisasi L2\n",
    "tfidf_matrix = vectorizer.fit_transform(df['stopword_removal'])\n",
    "\n",
    "# Mengubah hasilnya menjadi DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Menampilkan 10 baris pertama\n",
    "tfidf_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c88775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Asumsikan Anda memiliki DataFrame 'df' dengan kolom 'stopword_removal' dan 'kategori'\n",
    "# Buat fitur (X) dan label (y)\n",
    "X = tfidf_df\n",
    "y = df['kategori']  # Pastikan 'kategori' adalah nama kolom yang benar\n",
    "\n",
    "# Pisahkan dataset menjadi set pelatihan dan pengujian (80% pelatihan, 20% pengujian)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inisialisasi model Regresi Logistik\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Latih model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Lakukan prediksi pada set pengujian\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nHasil Klasifikasi:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "\n",
    "# Opsional, simpan model untuk digunakan nanti\n",
    "import joblib\n",
    "joblib.dump(model, 'model_regresi_logistik.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
